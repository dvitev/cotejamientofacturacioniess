{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cotejamiento de Cedulas Hospital General Milagro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A continuacion se realiza el cotejamiento de las cedulas de trabajadores y ex-trabajadores de la Empresa los Alamos segun Solicitud en Memorando Nro.IESS-CPPCG-2023-2144-A, contra los archivos planos del Hospital General Milagro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instalamos las librerias necesarias para procesar cantidades masivas de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importamos las librerias necesarias para los siguientes pasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T13:41:52.038664Z",
     "start_time": "2023-06-01T13:41:51.217419Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desactivamos los warnings(alertas) sobre tamaños masivos de datos, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtenemos he imprimimos la direccion de la carpeta actual de este notebook, que a su vez sera la direccion donde se va ha guardar todos los datos generados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T13:41:52.042723Z",
     "start_time": "2023-06-01T13:41:52.040160Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "columnas = ['DEPENDENCIA', 'CODIGO_ATENCION', 'FECHA_ATENCION', 'TIPO_DE_AFILIACION', 'CEDULA', 'NOMBRES', 'SEXO',\n",
    "            'FECHA_NACIMIENTO', 'EDAD', 'PROCEDIMIENTO', 'CODIGO_PROCEDI', 'DESCRIPCION', 'CIE_10', 'CIE10_2',\n",
    "            'CIE10_3', 'CANTIDAD', 'VALOR_UNITARIO', 'DURACION', 'PARENTESCO', 'IDENTIFICACION_AFILIADO',\n",
    "            'NOMBRES_AFILIADO', 'CODIGO_DERIVACION', 'DERIVACION_SECUENCIAL', 'CONTINGENCIA_CUBIERTA',\n",
    "            'DIAGNOSTICO_PRESUNTIVO_DEFINITIVO', 'TIEMPO_DE_ANESTESIA', 'VALOR_%_IVA', 'VALOR_UNITARIO_IVA',\n",
    "            'CODIGO_DE_LA_UNIDAD', 'CODIGO_PROFESIONAL', 'NOMBRE_PROFESIONAL', 'MARCA_FINAL_DE_LA_LINEA']\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El nombre del DataFrame que vamos a utilizar puede varias dependiendo de los criterios que se determinen, en este caso se le puso como nombre \"alamos\"\n",
    "#### Cargamos el archivo de Excel donde se encuentran las cedulas de los trabajadores de la Empresa los Alamos, por motivos practicos el nombre se lo dejo como el Numero de Memorando en el cual fue remitido el listado, el segundo parametro \"1\" es el nombre que tiene la hoja del libro de excel donde vamos a trabajar, Pandas obliga o colocar el nombre de la hoja en la que se va ha trajar en caso de que se carge un archivo de Excel con multiples hojas, a continuacion una recomendacion de los nombres para las columnas del archivo de excel par evitar errores por espacios o signos especiales:\n",
    "- ORD\n",
    "- CEDULA\n",
    "- NOMBRES_APELLIDOS\n",
    "- PERIODO\n",
    "- MEMORANDO_NRO\n",
    "- FECHA_PRESTACIONES_MEDICAS1 \n",
    "- DEPENDENCIA_SERVICIOS\n",
    "- VALOR_TOTAL\n",
    "- REGISTRA_SUBSIDIOS\n",
    "- DESDE\n",
    "- HASTA\n",
    "- VALOR_PRESTACIONES\n",
    "- OBSERVACIONES\n",
    "##### Nos Aseguramos de que la Columna 'CEDULA' este formateada en tipo texto con 10 caracteres y si no es asi ponga el \"0\" al inicio del numero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alamos = pd.read_excel(\"IESS-CPPCG-2023-2144-A.xls\",\"1\")\n",
    "# alamos = alamos.drop_duplicates(subset=['CEDULA'])\n",
    "alamos['CEDULA'] = alamos['CEDULA'].astype(str).str.zfill(10)\n",
    "print(len(alamos))\n",
    "alamos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como ayuda visual imprimimos los tipos de datos de cada columna de datos del DataFrame \"alamos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alamos.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedemos a hacer la carga masiva de los archivos planos (csv) facilitados por factuacion, es recomendable tener los archivos planos almacenados en una carpeta por año, con un nombre homogeneo, en este caso \"HG MILAGRO \" seguido del año (2017,2018,2019,2020,2021,etc); los nombres de cada archivo plano debe cumplir con ciertos requisitos como no tener espacios (sustituirlos con \"_\") el prefijo utilizado en este caso es \"HG_MI_\" mas el años y el mes al que representa ese archivo plano (\"HG_MI_201701\",etc), el prefijo a utilizarce es entera eleccion de cada persona pero el año y mes deben ser estrictamente en el formato \"yyyymm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T13:43:45.949899Z",
     "start_time": "2023-06-01T13:41:52.045275Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Crea un diccionario vacío para almacenar los datos\n",
    "datos_archivo_plano = {}\n",
    "subcarpetas = \"HG MILAGRO\"\n",
    "\n",
    "for a in range(2017,2022):\n",
    "    for archivo in os.listdir(os.path.join(path,f\"{subcarpetas} {a}\")):\n",
    "        if archivo.endswith('.csv'):\n",
    "            ruta_archivo = os.path.join(path, f\"{subcarpetas} {a}\", archivo)\n",
    "            nombre_archivo = archivo.split('_')[-1].split('.')[0][0:4] + '-' + archivo.split('_')[-1].split('.')[0][4:]\n",
    "            datos_archivo_plano[nombre_archivo] = pd.read_csv(ruta_archivo, sep=';', encoding='ISO-8859-1', header=None, names=columnas)\n",
    "            datos_archivo_plano[nombre_archivo]['VALOR_UNITARIO'] = datos_archivo_plano[nombre_archivo]['VALOR_UNITARIO'].replace(',','.', regex=True)\n",
    "            datos_archivo_plano[nombre_archivo]['VALOR_UNITARIO'] = datos_archivo_plano[nombre_archivo]['VALOR_UNITARIO'].astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como ayuda visual imprimimos los tipos de datos de cada columna de datos del DataFrame \"datos_archivo_plano\" del año 2017 del mes Febrero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_archivo_plano['2017-02'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nos Aseguramos de que la Columna 4:'CEDULA' y la columna 20:'IDENTIFICACION_AFILIADO' este formateada en tipo texto con 10 caracteres y si no es asi ponga el \"0\" al inicio del numero, en todos los archivos planos que hemos cargado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in datos_archivo_plano.keys():\n",
    "    datos_archivo_plano[key]['CEDULA'] = datos_archivo_plano[key]['CEDULA'].astype(str).str.zfill(10)\n",
    "    datos_archivo_plano[key]['IDENTIFICACION_AFILIADO'] = datos_archivo_plano[key]['IDENTIFICACION_AFILIADO'].astype(str).str.zfill(10)\n",
    "    # print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Realizamos el cotejamiento de cada una de las cedulas en el listado de trabajadores de la Empresa los Alamos, y lo verificamos contra la columan 20:'IDENTIFICACION_AFILIADO', la cual es el que acredita el derecho de atencion, utilizamos la comuna 'PERIODO' para limitar la busqueda a uno de los archivos planos previamente cargados; Y que las coincidencias que encuentre las separe en un DataFrame diferente para no alterar los datos originales puros; se imprime una ayuda visual para saber cuantas cedulas han sido procesadas ya que este paso puede demorar un poco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T13:44:16.766616Z",
     "start_time": "2023-06-01T13:44:16.608984Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dfAlamos = {}\n",
    "for (i,p) in alamos.iterrows():\n",
    "    key = p['PERIODO'].split('-')[0]+'-'+p['PERIODO'].split('-')[-1].zfill(2)\n",
    "    df = datos_archivo_plano[key].copy()\n",
    "    df2 = df[df['IDENTIFICACION_AFILIADO'].str.contains(p['CEDULA'])]\n",
    "    if key not in dfAlamos:\n",
    "        dfAlamos[key] = df2\n",
    "    else:\n",
    "        dfAlamos[key] = pd.concat([dfAlamos[key], df2])\n",
    "    if i%100==0:\n",
    "        print(f\"{i} de {len(alamos)}\", end=\" | \")\n",
    "print(f\"{len(alamos)} de {len(alamos)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imprimimos los años y mese de los archivos planos en lo que se hayan encontramos coincidencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfAlamos.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### en la carpeta \"periodos\" dentro de este proyecto se almacenaran los archivos planos filtrados con unicamente las coincidencias que se encontraron en los pasos anteriroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T13:51:51.856969Z",
     "start_time": "2023-06-01T13:51:51.840955Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for key in dfAlamos.keys():\n",
    "    # print(os.path.join(path,'periodos', key.replace('-','') + '.xlsx'))\n",
    "    dfAlamos[key].to_excel(os.path.join(path,'periodos', key.replace('-','') + '.xlsx'), index=False,header=columnas)\n",
    "    # print(key)\n",
    "print(\"Generados los Archivos Filtrados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Una vez obtenido el DataFrame \"dfAlamos\" con los datos filtrados, en el DataFrame \"alamos\" procedemos a insertar los datos filtrados en pasos anteriores, una vez que se consigue una coincidencia de la 'CEDULA' CON 'IDENTIFICACION_AFILIADO', se crea temporalmente la columan TOTAL la cual contendrá el resultado de CANTIDAD * VALOR_UNITARIO del archivo plano, se agrupa la coindencia por la 'FECHA_ATENCION' con la sumatoria de 'TOTAL', se inserta las fechas obtenidos en la columna 'FECHA_PRESTACIONES_MEDICAS1' en formato texto separadas por ' ' y se inserta la sumatoria total de todas las fechas en la columna 'VALOR_TOTAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, p in alamos.iterrows():\n",
    "    periodo = p['PERIODO'].split('-')[0] + '-' + p['PERIODO'].split('-')[-1].zfill(2)\n",
    "    df = dfAlamos.get(periodo)  # Buscar el DataFrame correspondiente a 'periodo'\n",
    "    \n",
    "    if df is not None:\n",
    "        df2 = df[df['IDENTIFICACION_AFILIADO'].str.contains(p['CEDULA'], na=False)]\n",
    "        \n",
    "        if not df2.empty:\n",
    "            df2['TOTAL'] = df2['CANTIDAD'] * df2['VALOR_UNITARIO']\n",
    "            df2['TOTAL'] = df2['TOTAL'].astype(float)\n",
    "            sumatoria = df2.groupby(['FECHA_ATENCION'])['TOTAL'].sum()\n",
    "            alamos.at[i, 'FECHA_PRESTACIONES_MEDICAS1'] = ' '.join(sumatoria.keys().values)\n",
    "            alamos.at[i, 'VALOR_TOTAL'] = str(sumatoria.sum()).replace('.', ',')\n",
    "        # print(' '.join(sumatoria.keys().values),str(sumatoria.sum()).replace('.',','))\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para asegurar la integridad del excel original con el listado de los Alamos, guardamos todos lo realizado en un neuvoe excel con el nombre se se crea conveniente en este caso \"alamos-201702-201812-procesado\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alamos.to_excel(os.path.join(path,'alamos-201702-201812-procesado.xlsx'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
